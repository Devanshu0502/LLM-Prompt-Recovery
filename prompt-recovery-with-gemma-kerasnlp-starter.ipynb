{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":67121,"databundleVersionId":7806901,"sourceType":"competition"},{"sourceId":7731345,"sourceType":"datasetVersion","datasetId":4517764},{"sourceId":7733314,"sourceType":"datasetVersion","datasetId":4518936},{"sourceId":11372,"sourceType":"modelInstanceVersion","modelInstanceId":5388}],"dockerImageVersionId":30665,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":1606.028568,"end_time":"2024-03-12T03:01:53.276238","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-03-12T02:35:07.247670","version":"2.5.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"20ea86fd2f224c039206022e77a77ac7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2688b74e4d3d4aedb0a691a1da204fc4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_91133f87aa9f49c0b8779552ee6d510d","placeholder":"​","style":"IPY_MODEL_20ea86fd2f224c039206022e77a77ac7","value":" 2000/2000 [00:00&lt;00:00, 19456.49it/s]"}},"4b540d96b3344b16b2c14b152fc9a08d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6107467b19d94625af0b08ae36bfeb38":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6f8ffeb5720243cda7eb8898fa162431","placeholder":"​","style":"IPY_MODEL_74bf24d1f9434efba0d87c4cdf88a60f","value":"100%"}},"61d7cd9f7a0f4334986c094e0706d21a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c2e7ff5566244d91a11a2ef20b9673dc","placeholder":"​","style":"IPY_MODEL_f1dc0e0f4451424e983806171fd1ea5d","value":"100%"}},"6735ef1b616b4ef8969359ded36edf26":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a09e62b705154d90993028e94b64f199","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_80798a8db40545e5939ffa62fa7daf8d","value":1}},"6a4263eb361646a8aa645774bde6c57e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_793302f4d2aa4ea6ac71c60651f7f370","max":2000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_93575588252c40ed937925b8dd02ffa8","value":2000}},"6f8ffeb5720243cda7eb8898fa162431":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74bf24d1f9434efba0d87c4cdf88a60f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"776d7e770bbc4b0eb5784eeb9d16ff29":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_61d7cd9f7a0f4334986c094e0706d21a","IPY_MODEL_6a4263eb361646a8aa645774bde6c57e","IPY_MODEL_2688b74e4d3d4aedb0a691a1da204fc4"],"layout":"IPY_MODEL_4b540d96b3344b16b2c14b152fc9a08d"}},"793302f4d2aa4ea6ac71c60651f7f370":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f4877d33cc74bdd96624a15a4a40532":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ef71b954300a4f8584993b1714681e24","placeholder":"​","style":"IPY_MODEL_c6524c6cfef44cb6a876c40ab77024c0","value":" 1/1 [00:00&lt;00:00,  1.91it/s]"}},"80798a8db40545e5939ffa62fa7daf8d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"91133f87aa9f49c0b8779552ee6d510d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"93575588252c40ed937925b8dd02ffa8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a09e62b705154d90993028e94b64f199":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8a49a037e474785a932a823b98403fc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b19a45c03e4341f3a86791c24f64fc22":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6107467b19d94625af0b08ae36bfeb38","IPY_MODEL_6735ef1b616b4ef8969359ded36edf26","IPY_MODEL_7f4877d33cc74bdd96624a15a4a40532"],"layout":"IPY_MODEL_a8a49a037e474785a932a823b98403fc"}},"c2e7ff5566244d91a11a2ef20b9673dc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6524c6cfef44cb6a876c40ab77024c0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ef71b954300a4f8584993b1714681e24":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f1dc0e0f4451424e983806171fd1ea5d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}},"version_major":2,"version_minor":0}}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<center><img src=\"https://keras.io/img/logo-small.png\" alt=\"Keras logo\" width=\"100\"><br/>\nThis starter notebook is provided by the Keras team.</center>","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.011498,"end_time":"2024-03-12T02:35:09.997818","exception":false,"start_time":"2024-03-12T02:35:09.986320","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# LLM Prompt Recovery with [KerasNLP](https://github.com/keras-team/keras-nlp) and [Keras](https://github.com/keras-team/keras)\n\n</div>\n\nIn this competition, the goal is to find the prompt used to transform a given text. Specifically, we're seeking the prompt or instruction used in the [Gemma 7B-it](https://www.kaggle.com/models/google/gemma/frameworks/pyTorch/variations/7b-it-quant) model to convert one text to another. Typically, large language models are instructed to transform one text to another style, but here we're tasked with the inverse: finding the instruction/prompt used for the transformation. This notebook walks you through fine-tuning the **Gemma 2b-it** model with LoRA for this prompt recovery task using KerasNLP. Witih KerasNLP, we can fine-tune with LoRA using just a few lines of code.\n\n**Fun fact**: This notebook is backend-agnostic, supporting TensorFlow, PyTorch, and JAX. However, the best performance can be achieved with `JAX`. KerasNLP and Keras enable the choice of preferred backend. Explore further details on [Keras](https://keras.io/keras_3/).\n\n**Note**: For a deeper understanding of KerasNLP, refer to the [KerasNLP guides](https://keras.io/keras_nlp/).","metadata":{"papermill":{"duration":0.010703,"end_time":"2024-03-12T02:35:10.019530","exception":false,"start_time":"2024-03-12T02:35:10.008827","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Import Libraries ","metadata":{"papermill":{"duration":0.010546,"end_time":"2024-03-12T02:35:10.040980","exception":false,"start_time":"2024-03-12T02:35:10.030434","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import os\nos.environ[\"KERAS_BACKEND\"] = \"jax\" # you can also use tensorflow or torch\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\" # avoid memory fragmentation on JAX backend.\n\nimport keras\nimport keras_nlp\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\ntqdm.pandas() # progress bar for pandas\n\nimport plotly.graph_objs as go\nimport plotly.express as px\nfrom IPython.display import display, Markdown","metadata":{"_kg_hide-output":true,"papermill":{"duration":14.805258,"end_time":"2024-03-12T02:35:24.856982","exception":false,"start_time":"2024-03-12T02:35:10.051724","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-15T09:03:02.369544Z","iopub.execute_input":"2024-03-15T09:03:02.369915Z","iopub.status.idle":"2024-03-15T09:03:16.297111Z","shell.execute_reply.started":"2024-03-15T09:03:02.369881Z","shell.execute_reply":"2024-03-15T09:03:16.296168Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-03-15 09:03:06.363120: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-15 09:03:06.363213: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-15 09:03:06.501723: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Configuration","metadata":{"papermill":{"duration":0.01096,"end_time":"2024-03-12T02:35:24.879280","exception":false,"start_time":"2024-03-12T02:35:24.868320","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class CFG:\n    seed = 42\n    dataset_path = \"/kaggle/input/llm-prompt-recovery\"\n    preset = \"gemma_instruct_2b_en\" # name of pretrained Gemma\n    sequence_length = 512 # max size of input sequence for training\n    batch_size = 1 # size of the input batch in training\n    epochs = 1 # number of epochs to train","metadata":{"papermill":{"duration":0.019492,"end_time":"2024-03-12T02:35:24.909945","exception":false,"start_time":"2024-03-12T02:35:24.890453","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-15T09:03:29.515830Z","iopub.execute_input":"2024-03-15T09:03:29.516981Z","iopub.status.idle":"2024-03-15T09:03:29.521438Z","shell.execute_reply.started":"2024-03-15T09:03:29.516948Z","shell.execute_reply":"2024-03-15T09:03:29.520490Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Reproducibility \nSets value for random seed to produce similar result in each run.","metadata":{"papermill":{"duration":0.010843,"end_time":"2024-03-12T02:35:24.931968","exception":false,"start_time":"2024-03-12T02:35:24.921125","status":"completed"},"tags":[]}},{"cell_type":"code","source":"keras.utils.set_random_seed(CFG.seed)","metadata":{"papermill":{"duration":0.018228,"end_time":"2024-03-12T02:35:24.961128","exception":false,"start_time":"2024-03-12T02:35:24.942900","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-15T09:03:30.671203Z","iopub.execute_input":"2024-03-15T09:03:30.671547Z","iopub.status.idle":"2024-03-15T09:03:30.676423Z","shell.execute_reply.started":"2024-03-15T09:03:30.671520Z","shell.execute_reply":"2024-03-15T09:03:30.675448Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Data\n\nNo training data is provided in this competition; in other words, we can use any openly available datasets for this competition. In this notebook, we will use two external datasets that utilize the **Gemma 7B** model to transform texts using prompts.\n\n**Data Format:**\n\nThese datasets includes:\n- `original_text`: Input text/essay that needs to be transformed.\n- `rewrite_prompt`: Prompt/Instruction that was used in the Gemma LM to transform `original_text`. This is also our **target** for this competition.\n- `rewritten_text`: Output text that was generated by the Gemma model.","metadata":{"papermill":{"duration":0.011902,"end_time":"2024-03-12T02:35:24.991294","exception":false,"start_time":"2024-03-12T02:35:24.979392","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# `LLM Prompt Recovery - Synthetic Datastore dataset` by @dschettler8845\ndf1 = pd.read_csv(\"/kaggle/input/llm-prompt-recovery-synthetic-datastore/gemma1000_w7b.csv\")\ndf1 = df1[[\"original_text\", \"rewrite_prompt\", \"gemma_7b_rewritten_text_temp0\"]]\ndf1 = df1.rename(columns={\"gemma_7b_rewritten_text_temp0\":\"rewritten_text\"})\ndf1.head(2)\n\n# `3000 Rewritten texts - Prompt recovery Challenge` by @dipamc77\ndf2 = pd.read_csv(\"/kaggle/input/3000-rewritten-texts-prompt-recovery-challenge/prompts_0_500_wiki_first_para_3000.csv\")\ndf2.head(2)\n\n# Merge all datasets\ndf = pd.concat([df1, df2], axis=0)\ndf = df.sample(2000).reset_index(drop=True) # to reduce training time we are only using 2k samples\ndf.head(5)","metadata":{"papermill":{"duration":0.277062,"end_time":"2024-03-12T02:35:25.279823","exception":false,"start_time":"2024-03-12T02:35:25.002761","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-15T09:03:32.329157Z","iopub.execute_input":"2024-03-15T09:03:32.329500Z","iopub.status.idle":"2024-03-15T09:03:32.566196Z","shell.execute_reply.started":"2024-03-15T09:03:32.329471Z","shell.execute_reply":"2024-03-15T09:03:32.565251Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                       original_text  \\\n0  Story highlights New findings suggest sperm al...   \n1  Glasgow's Needy is a non-registered charity or...   \n2  Pune: In a bizarre case, a parrot accused of \"...   \n3  Berrya is a genus of evergreen trees with fibr...   \n4  The Libertarian Party of Oregon is a political...   \n\n                                      rewrite_prompt  \\\n0                Turn this into a programmer's code.   \n1   Make this a formal apology letter to a customer.   \n2          Convert this into a technique to be used.   \n3  Convert the text into the layout of a classic ...   \n4  Recast it as the backstory for a mysterious an...   \n\n                                      rewritten_text  \n0  ```python\\n# Code to summarize the text\\n\\nspe...  \n1  [Your Name]\\n[Your Address]\\nGlasgow, Scotland...  \n2  **Technique:**\\n\\n**Step 1: Gather evidence.**...  \n3  ## Level Layout: Berrya Forest\\n\\n**Background...  \n4  In the shadows of the Evergreen State of Orego...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>original_text</th>\n      <th>rewrite_prompt</th>\n      <th>rewritten_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Story highlights New findings suggest sperm al...</td>\n      <td>Turn this into a programmer's code.</td>\n      <td>```python\\n# Code to summarize the text\\n\\nspe...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Glasgow's Needy is a non-registered charity or...</td>\n      <td>Make this a formal apology letter to a customer.</td>\n      <td>[Your Name]\\n[Your Address]\\nGlasgow, Scotland...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Pune: In a bizarre case, a parrot accused of \"...</td>\n      <td>Convert this into a technique to be used.</td>\n      <td>**Technique:**\\n\\n**Step 1: Gather evidence.**...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Berrya is a genus of evergreen trees with fibr...</td>\n      <td>Convert the text into the layout of a classic ...</td>\n      <td>## Level Layout: Berrya Forest\\n\\n**Background...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The Libertarian Party of Oregon is a political...</td>\n      <td>Recast it as the backstory for a mysterious an...</td>\n      <td>In the shadows of the Evergreen State of Orego...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Prompt Engineering\n\nHere's a simple prompt template we'll use to create instruction-response pairs from the `original_text`, `rewritten_text`, and `rewritten_prompt`:\n\n```\nInstruction:\nBelow, the `Original Text` passage has been rewritten/transformed/improved into `Rewritten Text` by the `Gemma 7b-it` LLM with a certain prompt/instruction. Your task is to carefully analyze the differences between the \"Original Text\" and \"Rewritten Text\", and try to infer the specific prompt or instruction that was likely given to the LLM to rewrite/transform/improve the text in this way.\n\nOriginal Text: \n...\n\nRewritten Text:\n...\n\nResponse:\n...\n```\n\nThis template will help the model to follow instruction and respond accurately. You can explore more advanced prompt templates for better results.","metadata":{"papermill":{"duration":0.0116,"end_time":"2024-03-12T02:35:25.303305","exception":false,"start_time":"2024-03-12T02:35:25.291705","status":"completed"},"tags":[]}},{"cell_type":"code","source":"template = \"\"\"Instruction:\\nBelow, the `Original Text` passage has been rewritten/transformed/improved into `Rewritten Text` by the `Gemma 7b-it` LLM with a certain prompt/instruction. Your task is to carefully analyze the differences between the `Original Text` and `Rewritten Text`, and try to infer the specific prompt or instruction that was likely given to the LLM to rewrite/transform/improve the text in this way.\\n\\nOriginal Text:\\n{original_text}\\n\\nRewriten Text:\\n{rewritten_text}\\n\\nResponse:\\n{rewrite_prompt}\"\"\"","metadata":{"papermill":{"duration":0.020191,"end_time":"2024-03-12T02:35:25.335163","exception":false,"start_time":"2024-03-12T02:35:25.314972","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-15T09:03:33.767813Z","iopub.execute_input":"2024-03-15T09:03:33.768688Z","iopub.status.idle":"2024-03-15T09:03:33.773232Z","shell.execute_reply.started":"2024-03-15T09:03:33.768651Z","shell.execute_reply":"2024-03-15T09:03:33.772230Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df[\"prompt\"] = df.progress_apply(lambda row: template.format(original_text=row.original_text,\n                                                             rewritten_text=row.rewritten_text,\n                                                             rewrite_prompt=row.rewrite_prompt), axis=1)\ndata = df.prompt.tolist()","metadata":{"papermill":{"duration":0.130926,"end_time":"2024-03-12T02:35:25.477815","exception":false,"start_time":"2024-03-12T02:35:25.346889","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-15T09:03:34.349709Z","iopub.execute_input":"2024-03-15T09:03:34.350652Z","iopub.status.idle":"2024-03-15T09:03:34.455898Z","shell.execute_reply.started":"2024-03-15T09:03:34.350610Z","shell.execute_reply":"2024-03-15T09:03:34.454992Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d6f00a56a3e40ac93937a8c98a87ed1"}},"metadata":{}}]},{"cell_type":"markdown","source":"Let's examine a sample prompt. As the answers in our dataset are curated with **markdown** format, we will render the sample using `Markdown()` to properly visualize the formatting.","metadata":{"papermill":{"duration":0.011131,"end_time":"2024-03-12T02:35:25.500670","exception":false,"start_time":"2024-03-12T02:35:25.489539","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Sample","metadata":{"papermill":{"duration":0.011955,"end_time":"2024-03-12T02:35:25.523954","exception":false,"start_time":"2024-03-12T02:35:25.511999","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def colorize_text(text):\n    for word, color in zip([\"Instruction\", \"Original Text\", \"Rewriten Text\", \"Response\"],\n                           [\"red\", \"yellow\", \"blue\", \"green\"]):\n        text = text.replace(f\"{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n    return text","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.018985,"end_time":"2024-03-12T02:35:25.554601","exception":false,"start_time":"2024-03-12T02:35:25.535616","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-15T09:03:36.037427Z","iopub.execute_input":"2024-03-15T09:03:36.037768Z","iopub.status.idle":"2024-03-15T09:03:36.042957Z","shell.execute_reply.started":"2024-03-15T09:03:36.037741Z","shell.execute_reply":"2024-03-15T09:03:36.041810Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Take a random sample\nsample = data[10]\n\n# Give colors to Instruction, Response and Category\nsample = colorize_text(sample)\n\n# Show sample in markdown\ndisplay(Markdown(sample))","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.019438,"end_time":"2024-03-12T02:35:25.585175","exception":false,"start_time":"2024-03-12T02:35:25.565737","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-15T09:03:36.853513Z","iopub.execute_input":"2024-03-15T09:03:36.854212Z","iopub.status.idle":"2024-03-15T09:03:36.860702Z","shell.execute_reply.started":"2024-03-15T09:03:36.854179Z","shell.execute_reply":"2024-03-15T09:03:36.859777Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"\n\n**<font color='red'>Instruction:</font>**\nBelow, the `Original Text` passage has been rewritten/transformed/improved into `Rewritten Text` by the `Gemma 7b-it` LLM with a certain prompt/instruction. Your task is to carefully analyze the differences between the `Original Text` and `Rewritten Text`, and try to infer the specific prompt or instruction that was likely given to the LLM to rewrite/transform/improve the text in this way.\n\n\n\n**<font color='yellow'>Original Text:</font>**\nZoltán Tóth (born 24 August 1979) is a Hungarian former competitive figure skater. He is a five-time Hungarian national champion and competed in two Winter Olympics.\n\n\n\n**<font color='blue'>Rewriten Text:</font>**\nSure, here's the rewritten text as if it were a list of ingredients in a magical potion:\n\nThe ingredients to create the potion:\n\nZoltán Tóth's birth date: 24 August 1979\nHungarian heritage\nFive-time Hungarian national champion\nTwo Winter Olympics participation\n\n\n\n**<font color='green'>Response:</font>**\nRewrite the text as if it were a list of ingredients in a magical potion."},"metadata":{}}]},{"cell_type":"markdown","source":"# Modeling\n\n<div align=\"center\"><img src=\"https://i.ibb.co/Bqg9w3g/Gemma-Logo-no-background.png\" width=\"300\"></div>\n\n**Gemma** is a collection of advanced open LLMs developed by **Google DeepMind** and other **Google teams**, derived from the same research and technology behind the **Gemini** models. They can be integrated into applications and run on various platforms including mobile devices and hosted services. Developers can customize Gemma models using tuning techniques to enhance their performance for specific tasks, offering more targeted and efficient generative AI solutions beyond text generation.\n\nGemma models are available in several sizes so we can build generative AI solutions based on your available computing resources, the capabilities you need, and where you want to run them.\n\n| Parameters size | Tuned versions    | Intended platforms                 | Preset                 |\n|-----------------|-------------------|------------------------------------|------------------------|\n| 2B              | Pretrained        | Mobile devices and laptops         | `gemma_2b_en`          |\n| 2B              | Instruction tuned | Mobile devices and laptops         | `gemma_instruct_2b_en` |\n| 7B              | Pretrained        | Desktop computers and small servers| `gemma_7b_en`          |\n| 7B              | Instruction tuned | Desktop computers and small servers| `gemma_instruct_7b_en` |\n\nIn this notebook, we will utilize the `Gemma 2b-it` model from KerasNLP's pretrained models to recover the prompt. We are using the \"Instruction tuned\" model instead of the \"Pretrained\" one because the test data was generated from an instruction-tuned Gemma model. Additionally, we will fine-tune our model using instruction-response pairs thus fine-tuning an instruction-tuned model will likely yield better results.\n\nTo explore other available models, you can simply adjust the `preset` value in the `CFG` (config). You can find a list of other pretrained models on the [KerasNLP website](https://keras.io/api/keras_nlp/models/).","metadata":{"papermill":{"duration":0.011565,"end_time":"2024-03-12T02:35:25.608346","exception":false,"start_time":"2024-03-12T02:35:25.596781","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Gemma Causal LM\n\nThe code below will build an end-to-end Gemma model for causal language modeling (hence the name `GemmaCausalLM`). A causal language model (LM) predicts the next token based on previous tokens. This task setup can be used to train the model unsupervised on plain text input or to autoregressively generate plain text similar to the data used for training. This task can be used for pre-training or fine-tuning a Gemma model simply by calling `fit()`.\n\nThis model has a `generate()` method, which generates text based on a prompt. The generation strategy used is controlled by an additional sampler argument on `compile()`. You can recompile the model with different `keras_nlp.samplers` objects to control the generation. By default, `\"greedy\"` sampling will be used.\n\n> The `from_preset` method instantiates the model from a preset architecture and weights.","metadata":{"papermill":{"duration":0.011305,"end_time":"2024-03-12T02:35:25.631363","exception":false,"start_time":"2024-03-12T02:35:25.620058","status":"completed"},"tags":[]}},{"cell_type":"code","source":"gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(CFG.preset)\ngemma_lm.summary()","metadata":{"papermill":{"duration":65.300646,"end_time":"2024-03-12T02:36:30.943762","exception":false,"start_time":"2024-03-12T02:35:25.643116","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-15T09:03:38.486002Z","iopub.execute_input":"2024-03-15T09:03:38.486828Z","iopub.status.idle":"2024-03-15T09:04:38.748645Z","shell.execute_reply.started":"2024-03-15T09:03:38.486796Z","shell.execute_reply":"2024-03-15T09:04:38.747660Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"Attaching 'config.json' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'config.json' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'model.weights.h5' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'tokenizer.json' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'assets/tokenizer/vocabulary.spm' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nnormalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │   \u001b[38;5;34m2,506,172,416\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m524,288,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"## Gemma LM Preprocessor\n\nAn important part of the Gemma model is the **Preprocessor** layer, which under the hood uses **Tokenizer**.\n\n**What it does:** The preprocessor takes input strings and transforms them into a dictionary (`token_ids`, `padding_mask`) containing preprocessed tensors. This process starts with tokenization, where input strings are converted into sequences of token IDs.\n\n**Why it's important:** Initially, raw text data is complex and challenging for modeling due to its high dimensionality. By converting text into a compact set of tokens, such as transforming `\"The quick brown fox\"` into `[\"the\", \"qu\", \"##ick\", \"br\", \"##own\", \"fox\"]`, we simplify the data. Many models rely on special tokens and additional tensors to understand input. These tokens help divide input and identify padding, among other tasks. Making all sequences the same length through padding boosts computational efficiency, making subsequent steps smoother.\n\nExplore the following pages to access the available preprocessing and tokenizer layers in **KerasNLP**:\n- [Preprocessing](https://keras.io/api/keras_nlp/preprocessing_layers/)\n- [Tokenizers](https://keras.io/api/keras_nlp/tokenizers/)","metadata":{"papermill":{"duration":0.01333,"end_time":"2024-03-12T02:36:30.970517","exception":false,"start_time":"2024-03-12T02:36:30.957187","status":"completed"},"tags":[]}},{"cell_type":"code","source":"x, y, sample_weight = gemma_lm.preprocessor(data[0:2])","metadata":{"papermill":{"duration":0.349305,"end_time":"2024-03-12T02:36:31.332828","exception":false,"start_time":"2024-03-12T02:36:30.983523","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-15T09:04:38.750174Z","iopub.execute_input":"2024-03-15T09:04:38.750467Z","iopub.status.idle":"2024-03-15T09:04:39.078924Z","shell.execute_reply.started":"2024-03-15T09:04:38.750441Z","shell.execute_reply":"2024-03-15T09:04:39.078094Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"This preprocessing layer will take in batches of strings, and return outputs in a `(x, y, sample_weight)` format, where the `y` label is the next token id in the `x` sequence.\n\nFrom the code below, we can see that, after the preprocessor, the data shape is `(num_samples, sequence_length)`.","metadata":{"papermill":{"duration":0.013485,"end_time":"2024-03-12T02:36:31.360361","exception":false,"start_time":"2024-03-12T02:36:31.346876","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Display the shape of each processed output\nfor k, v in x.items():\n    print(k, \":\", v.shape)","metadata":{"papermill":{"duration":0.021854,"end_time":"2024-03-12T02:36:31.395870","exception":false,"start_time":"2024-03-12T02:36:31.374016","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-15T09:05:11.226812Z","iopub.execute_input":"2024-03-15T09:05:11.227542Z","iopub.status.idle":"2024-03-15T09:05:11.232333Z","shell.execute_reply.started":"2024-03-15T09:05:11.227511Z","shell.execute_reply":"2024-03-15T09:05:11.231425Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"token_ids : (2, 8192)\npadding_mask : (2, 8192)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Inference before Fine-Tuning\n\nBefore we do fine-tuning, let's try to recover the prompt using the Gemma model with some prepared prompts and see how it responds.\n\n> As this model is not yet fine-tuned for instruction, you will notice that the model's responses are inaccurate.","metadata":{"papermill":{"duration":0.013765,"end_time":"2024-03-12T02:36:31.423542","exception":false,"start_time":"2024-03-12T02:36:31.409777","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Sample 1","metadata":{"papermill":{"duration":0.013028,"end_time":"2024-03-12T02:36:31.450927","exception":false,"start_time":"2024-03-12T02:36:31.437899","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Take one sample\nrow = df.iloc[10]\n\n# Generate Prompt using template\nprompt = template.format(\n    original_text=row.original_text,\n    rewritten_text=row.rewritten_text,\n    rewrite_prompt=\"\",\n)\n\n# Infer\noutput = gemma_lm.generate(prompt, max_length=512)\n\n# Colorize\noutput = colorize_text(output)\n\n# Display in markdown\ndisplay(Markdown(output))\n","metadata":{"_kg_hide-input":true,"papermill":{"duration":12.745482,"end_time":"2024-03-12T02:36:44.209470","exception":false,"start_time":"2024-03-12T02:36:31.463988","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-15T09:05:12.767632Z","iopub.execute_input":"2024-03-15T09:05:12.768373Z","iopub.status.idle":"2024-03-15T09:05:25.127560Z","shell.execute_reply.started":"2024-03-15T09:05:12.768341Z","shell.execute_reply":"2024-03-15T09:05:25.126615Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"\n\n**<font color='red'>Instruction:</font>**\nBelow, the `Original Text` passage has been rewritten/transformed/improved into `Rewritten Text` by the `Gemma 7b-it` LLM with a certain prompt/instruction. Your task is to carefully analyze the differences between the `Original Text` and `Rewritten Text`, and try to infer the specific prompt or instruction that was likely given to the LLM to rewrite/transform/improve the text in this way.\n\n\n\n**<font color='yellow'>Original Text:</font>**\nZoltán Tóth (born 24 August 1979) is a Hungarian former competitive figure skater. He is a five-time Hungarian national champion and competed in two Winter Olympics.\n\n\n\n**<font color='blue'>Rewriten Text:</font>**\nSure, here's the rewritten text as if it were a list of ingredients in a magical potion:\n\nThe ingredients to create the potion:\n\nZoltán Tóth's birth date: 24 August 1979\nHungarian heritage\nFive-time Hungarian national champion\nTwo Winter Olympics participation\n\n\n\n**<font color='green'>Response:</font>**\nThe LLM was likely given the prompt or instruction to create a list of ingredients for a magical potion, based on the information in the original text."},"metadata":{}}]},{"cell_type":"markdown","source":"## Sample 2","metadata":{"papermill":{"duration":0.013613,"end_time":"2024-03-12T02:36:44.236874","exception":false,"start_time":"2024-03-12T02:36:44.223261","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Take one sample\nrow = df.iloc[20]\n\n# Generate Prompt using template\nprompt = template.format(\n    original_text=row.original_text,\n    rewritten_text=row.rewritten_text,\n    rewrite_prompt=\"\",\n)\n\n# Infer\noutput = gemma_lm.generate(prompt, max_length=512)\n\n# Colorize\noutput = colorize_text(output)\n\n# Display in markdown\ndisplay(Markdown(output))\n","metadata":{"_kg_hide-input":true,"papermill":{"duration":5.166131,"end_time":"2024-03-12T02:36:49.416727","exception":false,"start_time":"2024-03-12T02:36:44.250596","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-15T09:05:25.129231Z","iopub.execute_input":"2024-03-15T09:05:25.129526Z","iopub.status.idle":"2024-03-15T09:05:30.234102Z","shell.execute_reply.started":"2024-03-15T09:05:25.129500Z","shell.execute_reply":"2024-03-15T09:05:30.233168Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"\n\n**<font color='red'>Instruction:</font>**\nBelow, the `Original Text` passage has been rewritten/transformed/improved into `Rewritten Text` by the `Gemma 7b-it` LLM with a certain prompt/instruction. Your task is to carefully analyze the differences between the `Original Text` and `Rewritten Text`, and try to infer the specific prompt or instruction that was likely given to the LLM to rewrite/transform/improve the text in this way.\n\n\n\n**<font color='yellow'>Original Text:</font>**\nAbu al-Hassan Ali ibn Moussa ibn Rashid al-Alami (), also known as Sherif Moulay Ali Ben Rachid, was the founder of the city of Chefchaouen, Morocco. He was an Idrisid and descendant of Sufi saint Abd as-Salam ibn Mashish al-Alami. He was also the father of Sayyida al-Hurra, governor of Tetouan.\n\n\n\n**<font color='blue'>Rewriten Text:</font>**\n## Internal Audit Checklist - Governance and Historical Background\n\n**Objective:** To assess the accuracy and completeness of the historical narrative regarding the founding of Chefchaouen and the related individuals' identities and achievements.\n\n**Criteria:**\n\n**1. Accuracy of the historical narrative:**\n\n- Does the text accurately describe Abu al-Hassan Ali ibn Moussa ibn Rashid al-Alami's role as the founder of Chefchaouen?\n- Does the text accurately describe the Idrisid heritage and Sufi lineage of the founder?\n- Is the information about the founder's relationship to Sayyida al-Hur\n\n\n\n**<font color='green'>Response:</font>**\n**Yes, the rewritten text is largely accurate and provides a good overview of the historical narrative.**\n\n**2. Completeness of the historical narrative:**\n\n- Does the text provide all the relevant historical information about the founding of Chefchaouen?\n- Are there any missing or omitted key details?\n- Is the text well-organized and easy to follow?\n\n\n\n**<font color='green'>Response:</font>**\n**The rewritten text provides a good overview of the historical narrative, but there are a few minor omissions and missing details that could be further addressed for greater accuracy.**\n\n**3. Clarity and conciseness of the rewritten text:**\n\n- Is the text written in a clear and concise style?\n- Is the information presented in a logical and easy-to-understand manner?\n- Does the text use appropriate vocabulary and tone?\n\n\n\n**<font color='green'>Response:</font>**\n**The rewritten text is well-written and clear, providing a concise and accurate overview of the historical narrative.**\n\n**4. Alignment with the original text"},"metadata":{}}]},{"cell_type":"markdown","source":"# Fine-tuning with LoRA\n\nTo get better responses from the model, we will fine-tune the model with Low Rank Adaptation (LoRA).\n\n**What exactly is LoRA?**\n\nLoRA is a method used to fine-tune large language models (LLMs) in an efficient way. It involves freezing the weights of the LLM and injecting trainable rank-decomposition matrices.\n\nImagine in an LLM, we have a pre-trained dense layer, represented by a $d \\times d$ weight matrix, denoted as $W_0$. We then initialize two additional dense layers, labeled as $A$ and $B$, with shapes $d \\times r$ and $r \\times d$, respectively. Here, $r$ denotes the rank, which is typically **much smaller than** $d$. Prior to LoRA, the model's output was computed using the equation $output = W_0 \\cdot x + b_0$, where $x$ represents the input and $b_0$ denotes the bias term associated with the original dense layer, which remains frozen. After applying LoRA, the equation becomes $output = (W_0 \\cdot x + b_0) + (B \\cdot A \\cdot x)$, where $A$ and $B$ denote the trainable rank-decomposition matrices that have been introduced.\n\n<center><img src=\"https://i.ibb.co/DWsbhLg/LoRA.png\" width=\"300\"><br/>\nCredit: <a href=\"https://arxiv.org/abs/2106.09685\">LoRA: Low-Rank Adaptation of Large Language Models</a> Paper</center>\n\n\nIn the LoRA paper, $A$ is initialized with $\\mathcal{N} (0, \\sigma^2)$ and $B$ with $0$, where $\\mathcal{N}$ denotes the normal distribution, and $\\sigma^2$ is the variance.\n\n**Why does LoRA save memory?**\n\nEven though we're adding more layers to the model with LoRA, it actually helps save memory. This is because the smaller layers (A and B) have fewer parameters to learn compared to the big model and fewer trainable parameters mean fewer optimizer variables to store. So, even though the overall model might seem bigger, it's actually more efficient in terms of memory usage. \n\n> This notebook uses a LoRA rank of `4`. A higher rank means more detailed changes are possible, but also means more trainable parameters.","metadata":{"papermill":{"duration":0.014451,"end_time":"2024-03-12T02:36:49.445622","exception":false,"start_time":"2024-03-12T02:36:49.431171","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Enable LoRA for the model and set the LoRA rank to 4.\ngemma_lm.backbone.enable_lora(rank=4)\ngemma_lm.summary()","metadata":{"papermill":{"duration":0.493479,"end_time":"2024-03-12T02:36:49.953541","exception":false,"start_time":"2024-03-12T02:36:49.460062","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-15T09:05:30.235194Z","iopub.execute_input":"2024-03-15T09:05:30.235472Z","iopub.status.idle":"2024-03-15T09:05:30.697968Z","shell.execute_reply.started":"2024-03-15T09:05:30.235447Z","shell.execute_reply":"2024-03-15T09:05:30.697100Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │   \u001b[38;5;34m2,507,536,384\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m524,288,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,507,536,384</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,507,536,384\u001b[0m (9.34 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,507,536,384</span> (9.34 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,363,968\u001b[0m (5.20 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,363,968</span> (5.20 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n</pre>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"**Notice** that, the number of trainable parameters is reduced from ~$2.5$ billions to ~$1.3$ millions after enabling LoRA.","metadata":{"papermill":{"duration":0.01467,"end_time":"2024-03-12T02:36:49.983272","exception":false,"start_time":"2024-03-12T02:36:49.968602","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Training","metadata":{"papermill":{"duration":0.014819,"end_time":"2024-03-12T02:36:50.012716","exception":false,"start_time":"2024-03-12T02:36:49.997897","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Limit the input sequence length to 512 (to control memory usage).\ngemma_lm.preprocessor.sequence_length = CFG.sequence_length \n\n# Compile the model with loss, optimizer, and metric\ngemma_lm.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=keras.optimizers.Adam(learning_rate=3e-5),\n    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)\n\n# Train model\ngemma_lm.fit(data, epochs=CFG.epochs, batch_size=CFG.batch_size)","metadata":{"papermill":{"duration":1479.936356,"end_time":"2024-03-12T03:01:29.964030","exception":false,"start_time":"2024-03-12T02:36:50.027674","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-15T09:05:30.699591Z","iopub.execute_input":"2024-03-15T09:05:30.699902Z","iopub.status.idle":"2024-03-15T09:30:08.593172Z","shell.execute_reply.started":"2024-03-15T09:05:30.699869Z","shell.execute_reply":"2024-03-15T09:30:08.592312Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1476s\u001b[0m 728ms/step - loss: 1.3144 - sparse_categorical_accuracy: 0.6193\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7a99ec152050>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Inference after fine-tuning\n\nLet's see how our fine-tuned model responds to the same questions we asked before fine-tuning the model.","metadata":{"papermill":{"duration":0.175571,"end_time":"2024-03-12T03:01:30.314251","exception":false,"start_time":"2024-03-12T03:01:30.138680","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Sample 1","metadata":{"papermill":{"duration":0.172611,"end_time":"2024-03-12T03:01:30.659870","exception":false,"start_time":"2024-03-12T03:01:30.487259","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Take one sample\nrow = df.iloc[10]\n\n# Generate Prompt using template\nprompt = template.format(\n    original_text=row.original_text,\n    rewritten_text=row.rewritten_text,\n    rewrite_prompt=\"\",\n)\n\n# Infer\noutput = gemma_lm.generate(prompt, max_length=512)\n\n# Colorize\noutput = colorize_text(output)\n\n# Display in markdown\ndisplay(Markdown(output))","metadata":{"_kg_hide-input":true,"papermill":{"duration":12.669481,"end_time":"2024-03-12T03:01:43.514660","exception":false,"start_time":"2024-03-12T03:01:30.845179","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-15T09:30:35.226242Z","iopub.execute_input":"2024-03-15T09:30:35.226972Z","iopub.status.idle":"2024-03-15T09:30:35.885015Z","shell.execute_reply.started":"2024-03-15T09:30:35.226942Z","shell.execute_reply":"2024-03-15T09:30:35.884062Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"\n\n**<font color='red'>Instruction:</font>**\nBelow, the `Original Text` passage has been rewritten/transformed/improved into `Rewritten Text` by the `Gemma 7b-it` LLM with a certain prompt/instruction. Your task is to carefully analyze the differences between the `Original Text` and `Rewritten Text`, and try to infer the specific prompt or instruction that was likely given to the LLM to rewrite/transform/improve the text in this way.\n\n\n\n**<font color='yellow'>Original Text:</font>**\nZoltán Tóth (born 24 August 1979) is a Hungarian former competitive figure skater. He is a five-time Hungarian national champion and competed in two Winter Olympics.\n\n\n\n**<font color='blue'>Rewriten Text:</font>**\nSure, here's the rewritten text as if it were a list of ingredients in a magical potion:\n\nThe ingredients to create the potion:\n\nZoltán Tóth's birth date: 24 August 1979\nHungarian heritage\nFive-time Hungarian national champion\nTwo Winter Olympics participation\n\n\n\n**<font color='green'>Response:</font>**\nMake it a list of ingredients to create a magical potion."},"metadata":{}}]},{"cell_type":"markdown","source":"## Sample 2","metadata":{"papermill":{"duration":0.17451,"end_time":"2024-03-12T03:01:43.864294","exception":false,"start_time":"2024-03-12T03:01:43.689784","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Take one sample\nrow = df.iloc[20]\n\n# Generate Prompt using template\nprompt = template.format(\n    original_text=row.original_text,\n    rewritten_text=row.rewritten_text,\n    rewrite_prompt=\"\",\n)\n\n# Infer\noutput = gemma_lm.generate(prompt, max_length=512)\n\n# Colorize\noutput = colorize_text(output)\n\n# Display in markdown\ndisplay(Markdown(output))\n","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.787106,"end_time":"2024-03-12T03:01:44.824264","exception":false,"start_time":"2024-03-12T03:01:44.037158","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-15T09:30:36.294989Z","iopub.execute_input":"2024-03-15T09:30:36.295866Z","iopub.status.idle":"2024-03-15T09:30:38.440587Z","shell.execute_reply.started":"2024-03-15T09:30:36.295818Z","shell.execute_reply":"2024-03-15T09:30:38.439637Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"\n\n**<font color='red'>Instruction:</font>**\nBelow, the `Original Text` passage has been rewritten/transformed/improved into `Rewritten Text` by the `Gemma 7b-it` LLM with a certain prompt/instruction. Your task is to carefully analyze the differences between the `Original Text` and `Rewritten Text`, and try to infer the specific prompt or instruction that was likely given to the LLM to rewrite/transform/improve the text in this way.\n\n\n\n**<font color='yellow'>Original Text:</font>**\nAbu al-Hassan Ali ibn Moussa ibn Rashid al-Alami (), also known as Sherif Moulay Ali Ben Rachid, was the founder of the city of Chefchaouen, Morocco. He was an Idrisid and descendant of Sufi saint Abd as-Salam ibn Mashish al-Alami. He was also the father of Sayyida al-Hurra, governor of Tetouan.\n\n\n\n**<font color='blue'>Rewriten Text:</font>**\n## Internal Audit Checklist - Governance and Historical Background\n\n**Objective:** To assess the accuracy and completeness of the historical narrative regarding the founding of Chefchaouen and the related individuals' identities and achievements.\n\n**Criteria:**\n\n**1. Accuracy of the historical narrative:**\n\n- Does the text accurately describe Abu al-Hassan Ali ibn Moussa ibn Rashid al-Alami's role as the founder of Chefchaouen?\n- Does the text accurately describe the Idrisid heritage and Sufi lineage of the founder?\n- Is the information about the founder's relationship to Sayyida al-Hur\n\n\n\n**<font color='green'>Response:</font>**\nConvert it into a comprehensive internal audit checklist that covers the following topics:\n\n1. Accuracy of the historical narrative \n2. Accuracy of the founding of Chefchaouen\n3. Accuracy of the founder's identity and lineage \n4. Accuracy of the founder's relationship to Sayyida al-Hurra"},"metadata":{}}]},{"cell_type":"markdown","source":"# Test Data","metadata":{"papermill":{"duration":0.224286,"end_time":"2024-03-12T03:01:45.223966","exception":false,"start_time":"2024-03-12T03:01:44.999680","status":"completed"},"tags":[]}},{"cell_type":"code","source":"test_df = pd.read_csv(\"/kaggle/input/llm-prompt-recovery/test.csv\")\ntest_df['original_text'] = test_df['original_text'].fillna(\"\")\ntest_df['rewritten_text'] = test_df['rewritten_text'].fillna(\"\")\ntest_df.head()","metadata":{"papermill":{"duration":0.200453,"end_time":"2024-03-12T03:01:45.600142","exception":false,"start_time":"2024-03-12T03:01:45.399689","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-15T09:30:38.442484Z","iopub.execute_input":"2024-03-15T09:30:38.442844Z","iopub.status.idle":"2024-03-15T09:30:38.456570Z","shell.execute_reply.started":"2024-03-15T09:30:38.442810Z","shell.execute_reply":"2024-03-15T09:30:38.455626Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"   id                                      original_text  \\\n0  -1  The competition dataset comprises text passage...   \n\n                                      rewritten_text  \n0  Here is your shanty: (Verse 1) The text is rew...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>original_text</th>\n      <th>rewritten_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-1</td>\n      <td>The competition dataset comprises text passage...</td>\n      <td>Here is your shanty: (Verse 1) The text is rew...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Test Sample\n\nNow, let's try out a sample from test data that model hasn't seen during training.","metadata":{"papermill":{"duration":0.178252,"end_time":"2024-03-12T03:01:45.955588","exception":false,"start_time":"2024-03-12T03:01:45.777336","status":"completed"},"tags":[]}},{"cell_type":"code","source":"row = test_df.iloc[0]\n\n# Generate Prompt using template\nprompt = template.format(\n    original_text=row.original_text,\n    rewritten_text=row.rewritten_text,\n    rewrite_prompt=\"\",\n)\n\n# Infer\noutput = gemma_lm.generate(prompt, max_length=512)\n\n# Colorize\noutput = colorize_text(output)\n\n# Display in markdown\ndisplay(Markdown(output))","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.87871,"end_time":"2024-03-12T03:01:47.008491","exception":false,"start_time":"2024-03-12T03:01:46.129781","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-15T09:30:39.923154Z","iopub.execute_input":"2024-03-15T09:30:39.923519Z","iopub.status.idle":"2024-03-15T09:30:40.443813Z","shell.execute_reply.started":"2024-03-15T09:30:39.923490Z","shell.execute_reply":"2024-03-15T09:30:40.442911Z"},"trusted":true},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"\n\n**<font color='red'>Instruction:</font>**\nBelow, the `Original Text` passage has been rewritten/transformed/improved into `Rewritten Text` by the `Gemma 7b-it` LLM with a certain prompt/instruction. Your task is to carefully analyze the differences between the `Original Text` and `Rewritten Text`, and try to infer the specific prompt or instruction that was likely given to the LLM to rewrite/transform/improve the text in this way.\n\n\n\n**<font color='yellow'>Original Text:</font>**\nThe competition dataset comprises text passages that have been rewritten by the Gemma LLM according to some rewrite_prompt instruction. The goal of the competition is to determine what prompt was used to rewrite each original text.  Please note that this is a Code Competition. When your submission is scored, this example test data will be replaced with the full test set. Expect roughly 2,000 original texts in the test set.\n\n\n\n**<font color='blue'>Rewriten Text:</font>**\nHere is your shanty: (Verse 1) The text is rewritten, the LLM has spun, With prompts so clever, they've been outrun. The goal is to find, the prompt so bright, To crack the code, and shine the light. (Chorus) Oh, this is a code competition, my dear, With text and prompts, we'll compete. Two thousand texts, a challenge grand, To guess the prompts, hand over hand.(Verse 2) The original text, a treasure lost, The rewrite prompt, a secret to be\n\n\n\n**<font color='green'>Response:</font>**\nTurn it into a code competition."},"metadata":{}}]},{"cell_type":"markdown","source":"# Submission","metadata":{"papermill":{"duration":0.175358,"end_time":"2024-03-12T03:01:47.370964","exception":false,"start_time":"2024-03-12T03:01:47.195606","status":"completed"},"tags":[]}},{"cell_type":"code","source":"preds = []\nfor i in tqdm(range(len(test_df))):\n    row = test_df.iloc[i]\n\n    # Generate Prompt using template\n    prompt = template.format(\n        original_text=row.original_text,\n        rewritten_text=row.rewritten_text,\n        rewrite_prompt=\"\"\n    )\n\n    # Infer\n    output = gemma_lm.generate(prompt, max_length=512)\n    pred = output.replace(prompt, \"\") # remove the prompt from output\n    \n    # Store predictions\n    preds.append([row.id, pred])","metadata":{"papermill":{"duration":0.708357,"end_time":"2024-03-12T03:01:48.254376","exception":false,"start_time":"2024-03-12T03:01:47.546019","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-15T09:30:41.378788Z","iopub.execute_input":"2024-03-15T09:30:41.379553Z","iopub.status.idle":"2024-03-15T09:30:42.026966Z","shell.execute_reply.started":"2024-03-15T09:30:41.379523Z","shell.execute_reply":"2024-03-15T09:30:42.025848Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7432f4eb32584cf990beac479dd6d9af"}},"metadata":{}}]},{"cell_type":"markdown","source":"While preparing the submission file, we must keep in mind that, leaving any `rewrite_prompt` blank as null answers will throw an error.","metadata":{"papermill":{"duration":0.184052,"end_time":"2024-03-12T03:01:48.628582","exception":false,"start_time":"2024-03-12T03:01:48.444530","status":"completed"},"tags":[]}},{"cell_type":"code","source":"sub_df = pd.DataFrame(preds, columns=[\"id\", \"rewrite_prompt\"])\nsub_df['rewrite_prompt'] = sub_df['rewrite_prompt'].fillna(\"\")\nsub_df['rewrite_prompt'] = sub_df['rewrite_prompt'].map(lambda x: \"Improve the essay\" if len(x) == 0 else x)\nsub_df.to_csv(\"submission.csv\",index=False)\nsub_df.head()","metadata":{"papermill":{"duration":0.199167,"end_time":"2024-03-12T03:01:49.003346","exception":false,"start_time":"2024-03-12T03:01:48.804179","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-15T09:30:42.603286Z","iopub.execute_input":"2024-03-15T09:30:42.603613Z","iopub.status.idle":"2024-03-15T09:30:42.618720Z","shell.execute_reply.started":"2024-03-15T09:30:42.603588Z","shell.execute_reply":"2024-03-15T09:30:42.617500Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"   id                                     rewrite_prompt\n0  -1  Turn this into the lyrics of a code competitio...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>rewrite_prompt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-1</td>\n      <td>Turn this into the lyrics of a code competitio...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Conclusion\n\nThe result is pretty good. Still there is ample room for improvement. Here are some tips to improve performance:\n\n- Try using the larger version of **Gemma** (7B).\n- Increase `sequence_length`.\n- Experiment with advanced prompt engineering techniques.\n- Implement augmentation to increase the number of samples.\n- Utilize a learning rate scheduler.","metadata":{"papermill":{"duration":0.176144,"end_time":"2024-03-12T03:01:49.368460","exception":false,"start_time":"2024-03-12T03:01:49.192316","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Reference\n* [Fine-tune Gemma models in Keras using LoRA](https://www.kaggle.com/code/nilaychauhan/fine-tune-gemma-models-in-keras-using-lora)\n* [Parameter-efficient fine-tuning of GPT-2 with LoRA](https://keras.io/examples/nlp/parameter_efficient_finetuning_of_gpt2_with_lora/)\n* [Gemma - KerasNLP](https://keras.io/api/keras_nlp/models/gemma/)","metadata":{"papermill":{"duration":0.17381,"end_time":"2024-03-12T03:01:49.715477","exception":false,"start_time":"2024-03-12T03:01:49.541667","status":"completed"},"tags":[]}}]}